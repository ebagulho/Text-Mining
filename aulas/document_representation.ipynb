{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representação de documentos e cálculo de similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gzip\n",
    "import numpy as np\n",
    "import glob\n",
    "import collections\n",
    "import operator\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso esteja a utilizar o google colab, antes de continuar certifique-se que fez o download dos dados previamente conforme instruções fornecidas em [download dos dados utilizados nos exemplos](./using-tm-data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    nltk.download('punkt')\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/Iscte/TM/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ficheiros=glob.glob(\"../data/pt/tsf.selecionado/*.gz\")\n",
    "print(ficheiros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = []\n",
    "for fname in ficheiros:\n",
    "    f=gzip.open(fname, \"rt\", encoding=\"utf-8\")\n",
    "    texto = f.readlines()\n",
    "    texto = \"\".join(texto)\n",
    "    textos.append(texto)\n",
    "print(\"Tenho guardados %d documentos\"% len(textos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textos[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos extrair contagens de palavras (features) a partir dos documentos.\n",
    "\n",
    "Nesta fase talvez não fosse má ideia fazer algum processamento, por exemplo, para passar a ter apenas palavras em letras minúsculas e evitar ter palavras no vocabulário tais como `«perfeito»` ou `«já`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2wc(texto):   \n",
    "    palavras = nltk.word_tokenize(texto.lower())\n",
    "    return collections.Counter(palavras)\n",
    "\n",
    "doc2wc(\"A capa do livro do Pedro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for texto in textos:\n",
    "    contagens = doc2wc(texto)\n",
    "    docs.append(contagens)\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talvez não fosse má ideia fazer algum processamento para, por exemplo:\n",
    " * passar a ter apenas palavras em letras minúsculas\n",
    " * eliminar tokens tais como `(`, `:`, `«`, etc...\n",
    " * evitar ter palavras no vocabulário tais como `«perfeito»` ou `«já`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora ver qual é o vocabulário que está a ser usado ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "voc = set()\n",
    "for d in docs:\n",
    "    for w in d:\n",
    "        voc.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"O vocabulário tem %d palavras\"% len(voc) )\n",
    "print(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Vamos representar os documentos usando apenas as palavras do vocabulário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora criar um identificador para cada palavra do vocabulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_index(voc):\n",
    "    voc_ids = {}\n",
    "    for i,w in enumerate(sorted(voc)):\n",
    "        voc_ids[w] = i\n",
    "    return voc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "voc_ids = create_index(voc)\n",
    "print(voc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora sim, vamos representar os nossos documentos em vetores numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(doc, voc_ids):\n",
    "    doc_vector = np.zeros([len(voc_ids)], dtype=int)\n",
    "    for w in doc:\n",
    "        if w in voc_ids:\n",
    "            doc_vector[ voc_ids[w] ] = doc[w]\n",
    "    return doc_vector\n",
    "\n",
    "docrep = []\n",
    "for doc in docs:\n",
    "    doc_vector = doc2vec(doc, voc_ids)\n",
    "    docrep.append ( doc_vector )\n",
    "    \n",
    "print(\"número de documentos:\", len(docrep))\n",
    "print(\"tamanho do documento 0 (é o mesmo para todos os documentos):\", len(docrep[0]))\n",
    "print(\"documento 0:\\n\", docrep[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos lá ver qual o documento mais próximo do segundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "ref=1\n",
    "for i in range(len(docrep)):\n",
    "    s = 1 - distance.cosine(docrep[ref], docrep[i])\n",
    "    print(f\"similarity({ref},{i}) is {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textos[ref])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textos[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF para identificar os termos mais relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(df, ndocs):\n",
    "    return np.log(ndocs/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = collections.Counter()\n",
    "df = collections.Counter()\n",
    "for d in docs:\n",
    "    for w in d:\n",
    "        tf[w] += d[w]\n",
    "        df[w] += 1\n",
    "        \n",
    "idfs = {}\n",
    "for w in tf:\n",
    "    if tf[w] > 1:\n",
    "        idfs[w] = idf(df[w], len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in [\"em\", \"com\", \"de\", \"uefa\", \"marcador\", \"voto\", \"cristiano\", \"tap\"]:\n",
    "    print(w, tf[w], df[w], len(docs), idf(df[w], len(docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(idfs.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que sabemos quais as palavras mais importantes, podemos definir novo vocabulário ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "voc2 = sorted(idfs, key=idfs.get, reverse=True)[:250]\n",
    "voc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos agora voltar a executar as instruções abaixo do ponto (1) e representar os documentos com este novo vocabulário. \n",
    "Poderá chegar à conclusão que o par de documentos mais similar não é, afinal, o 3 e o 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc2_ids = create_index(voc2)\n",
    "print(voc2_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docrep2 = []\n",
    "for doc in docs:\n",
    "    doc_vector = doc2vec(doc, voc2_ids)\n",
    "    docrep2.append ( doc_vector )\n",
    "    \n",
    "print(\"número de documentos:\", len(docrep2))\n",
    "print(\"tamanho do documento 0 (é o mesmo para todos os documentos):\", len(docrep2[0]))\n",
    "print(\"documento 0:\\n\", docrep2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref=3\n",
    "for i in range(len(docrep2)):\n",
    "    s = 1 - distance.cosine(docrep2[ref], docrep2[i])\n",
    "    print(f\"similarity({ref},{i}) is {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
