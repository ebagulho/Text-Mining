{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Existem já um conjunto de bibliotecas que permitem classificar um pedaço de texto quanto ao sentimento. Uma dessas bibliotecas é a \"textblob\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was good. ==> 0.7\n",
      "The movie was not good. ==> -0.35\n",
      "I really think this product sucks. ==> -0.04999999999999999\n",
      "Really great product. ==> 0.8\n",
      "I don't like this product ==> 0.0\n"
     ]
    }
   ],
   "source": [
    "texts=[\"The movie was good.\", \n",
    "    \"The movie was not good.\",\n",
    "    \"I really think this product sucks.\",\n",
    "    \"Really great product.\",\n",
    "    \"I don't like this product\"]\n",
    "for t in texts:\n",
    "    print(t, \"==>\", TextBlob(t).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code assumes that the text is already split into sentences, which may not be the case of texts comming from sources, such as *web pages* or *blogs*. An alternate solution would be to give the whole text to `textblob` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=TextBlob(\"\"\"The movie was good. The movie was not good. I really think this product sucks.\n",
    "Really great product. I don't like this product\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> The movie was good.\n",
      "=> The movie was not good.\n",
      "=> I really think this product sucks.\n",
      "=> Really great product.\n",
      "=> I don't like this product\n"
     ]
    }
   ],
   "source": [
    "for s in text.sentences:\n",
    "    print(\"=>\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was good. ==>  0.7\n",
      "The movie was not good. ==>  -0.35\n",
      "I really think this product sucks. ==>  -0.04999999999999999\n",
      "Really great product. ==>  0.8\n",
      "I don't like this product ==>  0.0\n"
     ]
    }
   ],
   "source": [
    "for s in text.sentences:\n",
    "    print(s, \"==> \", s.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating our own classifier\n",
    "Lets use [Sentiment Polarity Dataset 2.0](https://www.cs.cornell.edu/people/pabo/movie-review-data/), included in the `NLTK` library.<Br>\n",
    "It consists of 1000 positive and 1000 negative processed reviews. Introduced in Pang/Lee ACL 2004. Released June 2004."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 2000 reviews\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "print(\"The corpus contains %d reviews\"% len(mr.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg/cv995_23113.txt ==> neg\n",
      "neg/cv996_12447.txt ==> neg\n",
      "neg/cv997_5152.txt ==> neg\n",
      "neg/cv998_15691.txt ==> neg\n",
      "neg/cv999_14636.txt ==> neg\n",
      "pos/cv000_29590.txt ==> pos\n",
      "pos/cv001_18431.txt ==> pos\n",
      "pos/cv002_15918.txt ==> pos\n",
      "pos/cv003_11664.txt ==> pos\n",
      "pos/cv004_11636.txt ==> pos\n"
     ]
    }
   ],
   "source": [
    "for i in mr.fileids()[995:1005]: # Reviews 995 to 1005\n",
    "    print(i, \"==>\", i.split('/')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the content of one of these reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if anything , \" stigmata \" should be taken as a warning against releasing similarly-themed films relatively close to one another . \n",
      "of the four supernatural horror flicks released this year , it is clearly the worst . \n",
      "i suppose i should have seen this coming . \n",
      "after all , \" blair witch \" thoroughly creeped me out , \" sixth sense \" was mildly spooky , and then \" stir of echoes \" had its moments , but wasn't anything i'd lose sleep over . \n",
      "clearly , the quality of the horror this summer has slowly been dropping . \n",
      "is it then any surprise that \" stigmata \" is the dullest , most horribly executed piece of mtv-influenced tripe i have seen in a long while ? \n",
      "no , not really . \n",
      "patricia arquette plays frankie page , a hairdresser from pittsburgh who receives a rosary as a gift from her globe-trotting mother . \n",
      "as it turns out , the rosary belonged to a recently-deceased brazilian priest . \n",
      "the priest's church had been under investigation by father andrew kiernan ( gabriel byrne ) because of the mysterious appearance of a bleeding statue . \n",
      "father kiernan is an investigator who has made a career out of disproving supposed religious signs , but this time he believes there is something to the bleeding statue . \n",
      "his investigation is soon called off , however , when the frankie starts exhibiting signs of the stigmata , in which a person is inflicted with wounds like that of jesus christ . \n",
      "father kiernan is initially skeptical of frankie' s story , considering she's an atheist , but once he witnesses the stigmata attacks himself , he dedicates himself to finding out what is going on . \n",
      "he eventually begins to suspect that his boss , cardinal houseman ( jonathan pryce ) , is concealing something that could bring down the catholic church . \n",
      "as it turns out , the dead priest had been working on the translation of a \" fifth gospel \" before they were excommunicated . \n",
      "the new rupert wainwright music video . \n",
      " . \n",
      " . \n",
      "i mean film \" stigmata \" is the sort of film that starts off ok and only gets worse . \n",
      "indeed , it would have been good if it were a music video , because it's only interesting for about five minutes . \n",
      "i'm not sure what wainwright is trying to prove with his endless parade of slow-motion , double-exposure , and extreme close-ups ( whoaaaaaa ! ! ! ! ) , other than the fact that he has the most swelled head of any director in hollywood and , yes , he has been to film school . \n",
      "his camera trickery is interesting for a little while , but eventually , it becomes headache-inducing . \n",
      "what this film needed is a second audio track to be played over the dialogue ( such as it is ) , with wainwright screaming at the audience , \" see ? \n",
      "look at what i can do ! \n",
      "i'm an ex-cellent dir-ect-or . \" \n",
      "maybe then he'd explain why he decided to start half of his scenes with slow-motion shots of water dripping in reverse , or why he included random superfluous shots as that of an egg frying ( ooh , scary ! ) . \n",
      "if there was some underlying meaning behind all this camera trickery , i didn't see it . \n",
      "just when you thought it was safe to get involved in the story , here comes a double-exposure shot of two patricia arquettes collapsing into bed for no reason whatsoever . \n",
      "arrrrrrrghhhh . \n",
      " . \n",
      " . \n",
      "then again , the superfluous camera trickery wouldn't bother me if \" stigmata \" had a story or characters that were remotely engaging . \n",
      "though wainwright's vanity certainly doesn't help , he does seem to have been given a nearly unworkable script . \n",
      "where are plot continuity and character development when you need them ? \n",
      "case in point : frankie page is the character that ( i assume ) we are supposed to identify and sympathize with , but we aren't given any back story on her character , or any reason to like her . \n",
      "the extent of her character development seems to be that she is a hard-working hairdresser ( who can somehow afford a cavernous apartment on the top floor of her building ) and she's kind of cute , so let's start the bleeding ! \n",
      "the fact that she's clearly not the brightest bulb in the drawer doesn't help , either . \n",
      "according to frankie page , what is the first thing to do after receiving mysterious wounds on your wrists and back ? \n",
      "go clubbing ! \n",
      "sure , that makes sense . \n",
      "arquette , byrne , and pryce give it the old college try , but their characters are so one-dimensional that they just appear to be sleepwalking . \n",
      "scenes between arquette and byrne that were probably supposed to be sexually charged fall almost embarrassingly flat , because the setup of the romantic subplot is so clumsily handled that it almost reaches the point of becoming laughable . \n",
      "even though frankie page's life is falling apart before her very eyes , she still finds the time to hit on a priest who wanders in to her hair salon . \n",
      "even more curiously , he seems to be interested in her advances . \n",
      "father andrew's religious doubt pops up so suddenly that it seems more silly than dramatic . \n",
      "as for pryce , he may as well wear a curled mustache and cackle , \" i'll get you , my pretty , \" for all the depth his cardinal houseman is afforded . \n",
      "trust me , i'm not revealing anything by telling you pryce turns out to be a villain . \n",
      "what's worse , after being faced with dull characters and the prospect of having annoying camera tricks and loud music jammed down our throats , we now have to contend with a story that starts off in one direction , veers off in another , then another , and ends up being totally incomprehensible . \n",
      "first of all , the film doesn't even bother to explain what should be very simple plot details . how does frankie get the stigmata merely by touching a rosary ? \n",
      "how come an atheist like her was chosen , since father andrew mentions that only very devout believers have ever received stigmata ? \n",
      "actually , i'm not sure what frankie was posessed by . \n",
      "supposedly , stigmata occurs when one is posessed by the holy spirit , but the film later has her being posessed by the dead priest , and later by some evil spirit ( i think ) . \n",
      "which one is it ? \n",
      "the answer to this question , of course , is very simple : the possessing entity in each scene is determined by whichever effects and flashy camera work mr . wainwright wants to use this time . \n",
      "furthermore , the ending is a ridiculously neat little wrap-up , and the filmmakers compound this problem by ultimately turning the film into a diatribe against the catholic church . \n",
      "if you do any research at all about the gospel of st . thomas , you'll find that it is not being suppressed by the church ( as the film seems to claim ) , but that it is readily available at your local library . \n",
      "there's nothing the catholics need to worry about , though . \n",
      " \" stigmata's \" religion is so off-base that it can't be confused for anything remotely resembling the real catholic church . \n",
      "if they wanted to portray catholic priests as mobsters , they should have gone all-out and equipped them with sharkskin suits and tommy guns , which would have been far more interesting . \n",
      "i'm not catholic ; in fact , i don 't care much for the catholic church , but its cartoonish misrepresentation in this film should not be considered realistic by any means . \n",
      "it's rare to see a film that fails on as many levels as \" stigmata \" does . \n",
      "it' s not thought-provoking , though it would like to be , and it is definitely not scary , though it pretends to be . \n",
      "i'm not sure why they tried to pass this off as a horror film , because there is absolutely nothing scary about the story . \n",
      "maybe it's an attempt to cover up the fact that none of the scenes have any dramatic weight whatsoever . \n",
      "the initial shock of seeing arquette covered with blood is dulled by the fact that it happens over , and over , and over . \n",
      "like so many films of the mtv generation , this one suffers from overkill . \n",
      "so much is overdone in \" stigmata , \" that it eventually has no effect on the audience , leaving us to pick out the film's ( many ) flaws . \n",
      "i'm also still trying to figure out why the quotation they take from the gospel of st . thomas is so earth-shattering . \n",
      "when we finally hear it , the saying sounds like something every five-year-old learns on their first day of sunday school , which means that for all its flash , the whole film is really much ado about nothing . \n",
      "the dog days of summer usually produce one monstrous dog , and \" stigmata \" is it . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mr.raw(mr.fileids()[995]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the frequency of each word in the document ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 18, '.': 14, 'the': 13, 'a': 13, 'and': 8, 'of': 8, 'movie': 5, 'that': 4, 'in': 4, 'know': 4, ...})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "FreqDist(mr.raw(mr.fileids()[1]).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the most frequent words in the corpus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wordfreq = FreqDist()\n",
    "for i in mr.fileids():\n",
    "    wordfreq += FreqDist(w.lower() for w in mr.raw(i).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code has flaws because split() is a very basic way of finding the words. Let's use `word_tokenize()` or `mr.words()` instead..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordfreq = FreqDist()\n",
    "for i in mr.fileids():\n",
    "    wordfreq += FreqDist(w.lower() for w in mr.words(i))\n",
    "print(wordfreq)\n",
    "pprint(wordfreq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop words and punctuation are causing trouble, lets remove them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = stopwords.words('english')\n",
    "wordfreq = FreqDist()\n",
    "for i in mr.fileids():\n",
    "    wordfreq += FreqDist(w.lower() for w in mr.words(i) if w.lower() not in stopw and w.lower() not in string.punctuation)\n",
    "print(wordfreq)\n",
    "pprint(wordfreq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets shuffle the documents, otherwise they will remain sorted [\"neg\", \"neg\" ... \"pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "docnames=mr.fileids()\n",
    "random.shuffle(docnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split each document into words ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]\n",
    "for i in docnames:\n",
    "    y = i.split('/')[0]\n",
    "    documents.append( ( mr.words(i) , y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our documents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for docs in documents[:5]:\n",
    "    print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets produce the final document representation, in the form of a Frequency Distribution ...\n",
    "\n",
    "First, without stop words and punctuation ... (you could use other technique, such as IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = stopwords.words('english')\n",
    "docrep=[]\n",
    "for words,tag in documents:\n",
    "    features = FreqDist(w for w in words if w.lower() not in stopw and w.lower() not in string.punctuation)\n",
    "    docrep.append( (features, tag) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our documents again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docrep[:5]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK classifier: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining our training and test sets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numtrain = int(len(documents) * 80 / 100)  # number of training documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = docrep[:numtrain], docrep[numtrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier as nbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nbc.train(train_set)\n",
    "print(\"Accuracy:\", nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra forma de avaliar a Accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import scores\n",
    "test_ref = [tag for doc,tag in test_set]\n",
    "test_pred = classifier.classify_many([doc for doc,tag in test_set])\n",
    "print(\"Accuracy:\", scores.accuracy(test_pred, test_ref) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with vocabulary selection ..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "selected_features=[w for w,f in wordfreq.most_common(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_freq=FreqDist()\n",
    "for wordsf, t in docrep:\n",
    "    features_freq += wordsf\n",
    "features_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features=[f for f,ntimes in features_freq.most_common(500)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the word *frequency* in each document... (after executing, go back and test the performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [({w:f for w,f in wordsf.items() if w in selected_features}, tag) for wordsf,tag in docrep[:numtrain]]\n",
    "test_set = [({w:f for w,f in wordsf.items() if w in selected_features}, tag) for wordsf,tag in docrep[numtrain:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each one of the *selected_features*, use its frequency in each document... (after executing, go back and test the performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [({f:wordsf[f] for f in selected_features}, tag) for wordsf,tag in docrep[:numtrain]]\n",
    "test_set = [({f:wordsf[f] for f in selected_features}, tag) for wordsf,tag in docrep[numtrain:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie um novo modelo com esta nova representação e avalie o seu desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nbc.train(train_set)\n",
    "print(\"Accuracy:\", nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now with part-of-speech TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize(\"time flies like an arrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag([\"he\", \"flies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(documents[0][0])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = stopwords.words('english')\n",
    "docrep=[]\n",
    "for words,tag in documents:\n",
    "    features = FreqDist(\"%s_%s\"%(w,p) for w,p in nltk.pos_tag(words) if w.lower() not in stopw and w.lower() not in string.punctuation)\n",
    "    docrep.append( (features, tag) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docrep[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#pos_features = FreqDist(chain(*[doc for doc,tag in pos_docs]))\n",
    "#sel_pos_features=sorted(pos_features, key=pos_features.__getitem__, reverse=True)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posfeatures_freq=FreqDist()\n",
    "for ff, t in docrep:\n",
    "    posfeatures_freq += ff\n",
    "posfeatures_freq.most_common(10)\n",
    "selected_posfeatures=[f for f,freq in posfeatures_freq.most_common(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [({f:ff[f] for f in selected_posfeatures}, tag) for ff,tag in docrep[:numtrain]]\n",
    "test_set = [({f:ff[f] for f in selected_posfeatures}, tag) for ff,tag in docrep[numtrain:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the results again ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nbc.train(train_set)\n",
    "print(\"Accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício\n",
    "Calcule o desempenho do TextBlob usando o mesmo conjunto de teste.\n",
    "\n",
    "<!--\n",
    "y=[]\n",
    "y_pred=[]\n",
    "for fn in docnames[numtrain:]:\n",
    "    y.append(fn.split('/')[0])\n",
    "    if TextBlob(mr.raw(fn)).sentiment.polarity >= 0:\n",
    "        y_pred.append(\"pos\")\n",
    "    else:\n",
    "        y_pred.append(\"neg\")\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y, y_pred))\n",
    "-->"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now much faster, using some useful scikit-learn functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assuming that documents are shuffled\n",
    "Go back to the shuffle section, and make sure `docnames` contain a shuffled list of documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]\n",
    "tags = []\n",
    "for doc in docnames:\n",
    "    documents.append(mr.raw(doc))\n",
    "    tags.append( doc.split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    print(tags[i], documents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numtrain = int(len(documents) * 80 / 100)  # number of training documents\n",
    "train_documents, test_documents = documents[:numtrain], documents[numtrain:]\n",
    "train_tags, test_tags = tags[:numtrain], tags[numtrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_X = vectorizer.fit_transform(train_documents)\n",
    "test_X = vectorizer.transform(test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(train_X, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = metrics.accuracy_score(test_tags, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
