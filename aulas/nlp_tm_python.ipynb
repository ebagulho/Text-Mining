{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas úteis para NLP e Text Mining\n",
    "\n",
    "Este notebook permite ilustrar o funcionamento de algumas bibliotecas, tais como *NLTK*, *TextBlob* e *Spacy*. \n",
    "\n",
    "Para os mais curiosos, o website http://textanalysisonline.com permite também testar o funcionamento dessas bibliotecas, além de permitir também demonstrar o funcionamento de algumas tarefas comuns, tais como Análise de Sentimento. \n",
    "\n",
    "## NLTK\n",
    "NLTK is the most famous Python Natural Language Processing Toolkit. \n",
    "\n",
    "Check [Dive Into NLTK, Part I: Getting Started with NLTK](http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk) if you want to check some of theses functionalities in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pprint import pprint\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listas pré-definidas de palavras fechadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 207\n",
      "['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'às', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'é', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'era', 'eram', 'éramos', 'essa', 'essas', 'esse', 'esses', 'esta', 'está', 'estamos', 'estão', 'estar', 'estas', 'estava', 'estavam', 'estávamos', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estivéramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéssemos', 'estou', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'fôramos', 'forem', 'formos', 'fosse', 'fossem', 'fôssemos', 'fui', 'há', 'haja', 'hajam', 'hajamos', 'hão', 'havemos', 'haver', 'hei', 'houve', 'houvemos', 'houver']\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "print(\"total words:\", len(stopwords))\n",
    "print(stopwords[:90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Samsung recently revealed that it would give its phones\n",
    "an impressive four years of security updates. Paired\n",
    "with its high position in our regular Android update tracker, \n",
    "the company is rapidly becoming the Android update king. In \n",
    "light of the news, we're curious to know how\n",
    "up-to-date your phone might be. If you're willing \n",
    "to dig through the settings app for just a moment, let's\n",
    "compare: What Android security patch level is your phone running?\"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the most frequent tokens ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(tokens)\n",
    "print(freq.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech (POS)\n",
    "[Tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Massive iceberg, larger than New York City, breaks off in Antarctica\"\n",
    "print(nltk.pos_tag( text.split() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem as stem\n",
    "sentence = \"the flies died and denied their dead stating sensational lies\"\n",
    "\n",
    "stemmer = stem.porter.PorterStemmer()\n",
    "res = [ stemmer.stem(w) for w in sentence.split()]\n",
    "print(\" \".join(res) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos também `stemmers` mais sofisticados. Por exemplo, o SnowballStemmer também funciona para Português (apesar de ser mauzito) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Languages:\", stem.snowball.SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = \"lindos são os prados verdes e com muita luz deste Portugal\"\n",
    "stemmer = stem.snowball.SnowballStemmer(\"portuguese\")\n",
    "res = [stemmer.stem(w) for w in frase.split()]\n",
    "print(\" \".join(res) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos acesso a textos em português, que podemos usar para treinar e testar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import floresta\n",
    "print(\"Contains %s words\" % len(floresta.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.corpus.floresta.sents()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Samsung recently revealed that it would give its phones\n",
    "an impressive four years of security updates. Paired with its high\n",
    "position in our regular Android update tracker, the company\n",
    "is rapidly becoming the Android update king. In light of the news,\n",
    "we're curious to know how up-to-date your phone might be.\n",
    "If you're willing to dig through the settings app for just a moment,\n",
    "let's compare: What Android security patch level is your phone running?\"\"\"\n",
    "text = text.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token, lemma, POS, PT tag, Dependency tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:10]:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(f\"{chunk.text:35} | {chunk.root.text} | {chunk.root.dep_} | {chunk.root.head.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:15} {ent.label_:10} {ent.start_char} {ent.end_char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"\"\"verificaram-se crescimentos económicos em grande escala\"\"\")\n",
    "print(\" | \".join([token.text for token in doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemma, POS, Tag, Dependency Tag, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TextBlob\n",
    "TextBlob is a new python natural language processing toolkit, which stands on the shoulders of giants like NLTK and Pattern, provides text mining, text analysis and text processing modules for python developers.<BR>\n",
    "Material baseado em: http://textminingonline.com/getting-started-with-textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentação de um texto em frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) deals with the application of computational models to text or speech data. Application areas within NLP include automatic (machine) translation between languages; dialogue systems, which allow a human to interact with a machine using natural language; and information extraction, where the goal is to transform unstructured text into structured (database)  representations that can be searched and browsed in flexible ways. I love all the languages.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"\"\"Natural language processing (NLP) deals with\n",
    "the application of computational models to text or speech\n",
    "data. Application areas within NLP include automatic\n",
    "(machine) translation between languages; dialogue systems,\n",
    "which allow a human to interact with a machine using natural\n",
    "language; and information extraction, where the goal is to\n",
    "transform unstructured text into structured (database) \n",
    "representations that can be searched and browsed in flexible\n",
    "ways. I love all the languages.\"\"\"\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"Natural language processing (NLP) deals with the application of computational models to text or speech data.\"), Sentence(\"Application areas within NLP include automatic (machine) translation between languages; dialogue systems, which allow a human to interact with a machine using natural language; and information extraction, where the goal is to transform unstructured text into structured (database)  representations that can be searched and browsed in flexible ways.\"), Sentence(\"I love all the languages.\")]\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(text)\n",
    "print(blob.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, frase in enumerate(blob.sentences):\n",
    "    print(f\"{num} => {frase}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Samsung recently revealed that it would give its phones\n",
    "an impressive four years of security updates. Paired with its high\n",
    "position in our regular Android update tracker, the company\n",
    "is rapidly becoming the Android update king. In light of the news,\n",
    "we're curious to know how up-to-date your phone might be.\n",
    "If you're willing to dig through the settings app for just a moment,\n",
    "let's compare: What Android security patch level is your phone running?\"\"\"\n",
    "text = text.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(text)\n",
    "print(blob.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blob.noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in blob.sentences:\n",
    "    print(\"-->\", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Análise de sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an amazing library! --> 0.7500000000000001\n"
     ]
    }
   ],
   "source": [
    "frase = TextBlob(\"This is an amazing library!\")\n",
    "print(frase, \"-->\", frase.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos=[\"I love chocolate\", \"I hate to eat\", \"I don't love\", \"I like cakes\"]\n",
    "tags=[\"pos\", \"neg\", \"neg\", \"pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love chocolate 0.5 pos\n",
      "I hate to eat -0.8 neg\n",
      "I don't love 0.5 neg\n",
      "I like cakes 0.0 pos\n",
      "certos: 3, total de errados: 1, accuracy 0.75\n"
     ]
    }
   ],
   "source": [
    "certos=0\n",
    "errados=0\n",
    "for i in range(len(textos)):\n",
    "    x = TextBlob(textos[i])\n",
    "    polaridade = x.sentiment.polarity\n",
    "    print(textos[i], polaridade, tags[i])\n",
    "    if polaridade >=0 and tags[i] == \"pos\":\n",
    "        certos +=1\n",
    "    elif polaridade <0 and tags[i] == \"neg\":\n",
    "        certos += 1\n",
    "    else:\n",
    "        errados += 1\n",
    "acc=(certos)/(certos+errados)\n",
    "print(f\"certos: {certos}, total de errados: {errados}, accuracy {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curiosidade: o TextBlob também permite treinar um classificador (opcional)\n",
    "Additional references: [Tutorial: Building a Text Classification System](https://textblob.readthedocs.io/en/dev/classifiers.html#loading-data-and-creating-a-classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\n",
    "     ('I love this sandwich.', 'pos'),\n",
    "     ('this is an amazing place!', 'pos'),\n",
    "     ('I feel very good about these beers.', 'pos'),\n",
    "     ('this is my best work.', 'pos'),\n",
    "     (\"what an awesome view\", 'pos'),\n",
    "     ('I do not like this restaurant', 'neg'),\n",
    "     ('I am tired of this stuff.', 'neg'),\n",
    "     (\"I can't deal with this\", 'neg'),\n",
    "     ('he is my sworn enemy!', 'neg'),\n",
    "     ('my boss is horrible.', 'neg')\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.classify(\"This is an amazing library!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
