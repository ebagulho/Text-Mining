{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas úteis para NLP e Text Mining\n",
    "\n",
    "Este notebook permite ilustrar o funcionamento de algumas bibliotecas, tais como *NLTK*, *TextBlob* e *Spacy*. \n",
    "\n",
    "Para os mais curiosos, o website http://textanalysisonline.com permite também testar o funcionamento dessas bibliotecas, além de permitir também demonstrar o funcionamento de algumas tarefas comuns, tais como Análise de Sentimento. \n",
    "\n",
    "## NLTK\n",
    "NLTK is the most famous Python Natural Language Processing Toolkit. \n",
    "\n",
    "Check [Dive Into NLTK, Part I: Getting Started with NLTK](http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk) if you want to check some of theses functionalities in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listas pré-definidas de palavras fechadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "print(\"total words:\", len(stopwords))\n",
    "print(stopwords[:90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Samsung recently revealed that it would give its phones\n",
    "an impressive four years of security updates. Paired\n",
    "with its high position in our regular Android update tracker, \n",
    "the company is rapidly becoming the Android update king. In \n",
    "light of the news, we're curious to know how\n",
    "up-to-date your phone might be. If you're willing \n",
    "to dig through the settings app for just a moment, let's\n",
    "compare: What Android security patch level is your phone running?\"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the most frequent tokens ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(tokens)\n",
    "print(freq.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech (POS)\n",
    "O Conjunto de etiquetas utilizado nos próximos exemplos encontra-se descrito aqui: [Tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Massive iceberg, larger than New York City, breaks off in Antarctica\"\n",
    "print(nltk.pos_tag( text.split() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados parecem-lhe corretos? Como faria para melhorar a tokenização da frase e obter etiquetas mais adequadas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem as stem\n",
    "sentence = \"the flies died and denied their dead stating sensational lies\"\n",
    "\n",
    "stemmer = stem.porter.PorterStemmer()\n",
    "res = [ stemmer.stem(w) for w in sentence.split()]\n",
    "print(\" \".join(res) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos também `stemmers` mais sofisticados. Por exemplo, o SnowballStemmer também funciona para Português (apesar de ser mauzito) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Languages:\", stem.snowball.SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = \"lindos são os prados verdes e com muita luz deste Portugal\"\n",
    "stemmer = stem.snowball.SnowballStemmer(\"portuguese\")\n",
    "res = [stemmer.stem(w) for w in frase.split()]\n",
    "print(\" \".join(res) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos acesso a textos em português, que podemos usar para treinar e testar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import floresta\n",
    "print(\"Contains %s words\" % len(floresta.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.corpus.floresta.sents()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Samsung recently revealed that it would give its phones\n",
    "an impressive four years of security updates. Paired with its high\n",
    "position in our regular Android update tracker, the company\n",
    "is rapidly becoming the Android update king. In light of the news,\n",
    "we're curious to know how up-to-date your phone might be.\n",
    "If you're willing to dig through the settings app for just a moment,\n",
    "let's compare: What Android security patch level is your phone running?\"\"\"\n",
    "text = text.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token, lemma, POS, PT tag, Dependency tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:10]:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(f\"{chunk.text:35} | {chunk.root.text} | {chunk.root.dep_} | {chunk.root.head.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:15} {ent.label_:10} {ent.start_char} {ent.end_char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"\"\"verificaram-se crescimentos económicos em grande escala\"\"\")\n",
    "print(\" | \".join([token.text for token in doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemma, POS, Tag, Dependency Tag, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TextBlob\n",
    "TextBlob is a new python natural language processing toolkit, which stands on the shoulders of giants like NLTK and Pattern, provides text mining, text analysis and text processing modules for python developers.<BR>\n",
    "Material baseado em: http://textminingonline.com/getting-started-with-textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentação de um texto em frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"\"\"Natural language processing (NLP) deals with\n",
    "the application of computational models to text or speech\n",
    "data. Application areas within NLP include automatic\n",
    "(machine) translation between languages; dialogue systems,\n",
    "which allow a human to interact with a machine using natural\n",
    "language; and information extraction, where the goal is to\n",
    "transform unstructured text into structured (database) \n",
    "representations that can be searched and browsed in flexible\n",
    "ways. I love all the languages.\"\"\"\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(text)\n",
    "print(blob.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, frase in enumerate(blob.sentences):\n",
    "    print(f\"{num} => {frase}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Samsung recently revealed that it would give its phones\n",
    "an impressive four years of security updates. Paired with its high\n",
    "position in our regular Android update tracker, the company\n",
    "is rapidly becoming the Android update king. In light of the news,\n",
    "we're curious to know how up-to-date your phone might be.\n",
    "If you're willing to dig through the settings app for just a moment,\n",
    "let's compare: What Android security patch level is your phone running?\"\"\"\n",
    "text = text.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(text)\n",
    "print(blob.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blob.noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in blob.sentences:\n",
    "    print(\"-->\", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Análise de sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = TextBlob(\"This is an amazing library!\")\n",
    "print(frase, \"-->\", frase.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos=[\"I love chocolate\", \"I hate to eat\", \"I don't love\", \"I like cakes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(textos)):\n",
    "    polaridade = TextBlob(textos[i]).sentiment.polarity\n",
    "    print(textos[i], \"-->\", polaridade)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
