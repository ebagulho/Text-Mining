{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas para Text Mining e NLP\n",
    "O site http://textanalysisonline.com permite testar online algum do funcionamento das bibliotecas: NLTK, TextBlob, Spacy\n",
    "\n",
    "## NLTK\n",
    "NLTK is the most famous Python Natural Language Processing Toolkit. Check [Dive Into NLTK, Part I: Getting Started with NLTK](http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk) if you want to check some of theses functionalities in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listas pré-definidas de palavras fechadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "print(\"total words:\", len(stopwords), stopwords[:90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Samsung recently revealed that it would give its phones\n",
    "an impressive four years of security updates. Paired\n",
    "with its high position in our regular Android update tracker, \n",
    "the company is rapidly becoming the Android update king. In \n",
    "light of the news, we're curious to know how\n",
    "up-to-date your phone might be. If you're willing \n",
    "to dig through the settings app for just a moment, let's\n",
    "compare: What Android security patch level is your phone running?\"\"\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "print()\n",
    "fd = nltk.FreqDist(tokens)\n",
    "print(fd.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Massive iceberg, larger than New York City, breaks off in Antarctica\"\n",
    "print(nltk.pos_tag( text.split() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem as stem\n",
    "sentence = \"the flies died and denied their dead stating sensational lies\"\n",
    "\n",
    "stemmer = stem.porter.PorterStemmer()\n",
    "res = [ stemmer.stem(w) for w in sentence.split()]\n",
    "print(\" \".join(res) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos também `stemmers` mais sofisticados. Por exemplo, o SnowballStemmer também funciona para Português (apesar de ser mauzito) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Languages:\", stem.snowball.SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = \"lindos são os prados verdes e com muita luz deste Portugal\"\n",
    "stemmer = stem.snowball.SnowballStemmer(\"portuguese\")\n",
    "res = [stemmer.stem(w) for w in frase.split()]\n",
    "print(\" \".join(res) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos acesso a textos em português, que podemos usar para treinar e testar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import floresta\n",
    "print(\"Contains %s words\" % len(floresta.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.corpus.floresta.sents()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(\"\"\"Samsung recently revealed that it would give its phones\n",
    "an impressive four years of security updates. Paired\n",
    "with its high position in our regular Android update tracker, \n",
    "the company is rapidly becoming the Android update king. In \n",
    "light of the news, we're curious to know how\n",
    "up-to-date your phone might be. If you're willing \n",
    "to dig through the settings app for just a moment, let's\n",
    "compare: What Android security patch level is your phone running?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "doc = nlp(\"\"\"verificaram-se crescimentos económicos em grande escala\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" | \".join([token.text for token in doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemma, POS, Tag, Dependency Tag, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(f\"{chunk.text:40} | {chunk.root.text} | {chunk.root.dep_} | {chunk.root.head.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:15} {ent.label_:10} {ent.start_char} {ent.end_char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob\n",
    "TextBlob is a new python natural language processing toolkit, which stands on the shoulders of giants like NLTK and Pattern, provides text mining, text analysis and text processing modules for python developers.<BR>\n",
    "Material baseado em: http://textminingonline.com/getting-started-with-textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Samsung recently revealed that it would give its phones\n",
    "an impressive four years of security updates. Paired\n",
    "with its high position in our regular Android update tracker, \n",
    "the company is rapidly becoming the Android update king. In \n",
    "light of the news, we're curious to know how\n",
    "up-to-date your phone might be. If you're willing \n",
    "to dig through the settings app for just a moment, let's\n",
    "compare: What Android security patch level is your phone running?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(text)\n",
    "print(blob.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blob.noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in blob.sentences:\n",
    "    print(\"-->\", s.replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blob.translate(to=\"fr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinar um classificador (opcional)\n",
    "Additional references: [Tutorial: Building a Text Classification System](https://textblob.readthedocs.io/en/dev/classifiers.html#loading-data-and-creating-a-classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\n",
    "     ('I love this sandwich.', 'pos'),\n",
    "     ('this is an amazing place!', 'pos'),\n",
    "     ('I feel very good about these beers.', 'pos'),\n",
    "     ('this is my best work.', 'pos'),\n",
    "     (\"what an awesome view\", 'pos'),\n",
    "     ('I do not like this restaurant', 'neg'),\n",
    "     ('I am tired of this stuff.', 'neg'),\n",
    "     (\"I can't deal with this\", 'neg'),\n",
    "     ('he is my sworn enemy!', 'neg'),\n",
    "     ('my boss is horrible.', 'neg')\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.classify(\"This is an amazing library!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
