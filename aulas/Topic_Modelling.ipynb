{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics: health and sugar\n",
    "doc1 = \"sugar is bad to health.\"\n",
    "doc5 = \"health experts say that sugar is not good for your lifestyle.\"\n",
    "doc11 = \"my health is important, so I don't use sugar.\"\n",
    "doc12 = \"a good lifestyle means less blood pressure and a long life.\"\n",
    "doc13 = \"my life is important to me. so I practice sports.\"\n",
    "doc19 = \"My sister likes to have sugar, but not my father\"\n",
    "\n",
    "# driving \n",
    "doc2 = \"my father is driving my sister around to dance practice by car.\"\n",
    "doc7 = \"my father does not need driving me there.\"\n",
    "doc14 = \"I love driving my car.\"\n",
    "doc15 = \"I am driving to relax. I really love my car\"\n",
    "doc16 = \"driving my sister home is realy nice\"\n",
    "\n",
    "# school\n",
    "doc3 = \"my school is great, I love to study there\"\n",
    "doc4 = \"sometimes I feel happy to perform well at school.\"\n",
    "doc6 = \"I am doing well at school, but my sister could study a little more.\"\n",
    "doc17 = \"I study everyday, and I love the school.\"\n",
    "doc18 = \"my school is the best.\"\n",
    "\n",
    "# compile documents\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5, doc6, doc7, doc11, doc12, doc13, doc14, doc15, doc16, doc17, doc18, doc19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sugar', 'bad', 'health'], ['father', 'driving', 'sister', 'around', 'dance', 'practice', 'car'], ['school', 'great', 'love', 'study'], ['sometimes', 'feel', 'happy', 'perform', 'well', 'school'], ['health', 'expert', 'say', 'sugar', 'good', 'lifestyle'], ['well', 'school', 'sister', 'could', 'study', 'little', 'more'], ['father', 'need', 'driving', 'there'], ['health', 'important', 'use', 'sugar'], ['good', 'lifestyle', 'mean', 'le', 'blood', 'pressure', 'long', 'life'], ['life', 'important', 'me', 'practice', 'sport'], ['love', 'driving', 'car'], ['driving', 'relax', 'really', 'love', 'car'], ['driving', 'sister', 'home', 'realy', 'nice'], ['study', 'everyday', 'love', 'school'], ['school', 'best'], ['sister', 'like', 'sugar', 'father']]\n"
     ]
    }
   ],
   "source": [
    "print(doc_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)], [(3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(10, 1), (11, 1), (12, 1), (13, 1)], [(12, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)], [(1, 1), (2, 1), (19, 1), (20, 1), (21, 1), (22, 1)], [(9, 1), (12, 1), (13, 1), (18, 1), (23, 1), (24, 1), (25, 1)], [(6, 1), (7, 1), (26, 1), (27, 1)], [(1, 1), (2, 1), (28, 1), (29, 1)], [(20, 1), (21, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1)], [(8, 1), (28, 1), (32, 1), (36, 1), (37, 1)], [(4, 1), (6, 1), (11, 1)], [(4, 1), (6, 1), (11, 1), (38, 1), (39, 1)], [(6, 1), (9, 1), (40, 1), (41, 1), (42, 1)], [(11, 1), (12, 1), (13, 1), (43, 1)], [(12, 1), (44, 1)], [(2, 1), (7, 1), (9, 1), (45, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint  \n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.087*\"driving\" + 0.087*\"car\" + 0.061*\"love\" + 0.060*\"school\"'),\n",
      " (1, '0.065*\"sister\" + 0.065*\"father\" + 0.064*\"sugar\" + 0.064*\"driving\"'),\n",
      " (2, '0.069*\"school\" + 0.069*\"study\" + 0.049*\"health\" + 0.048*\"good\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint  \n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=100)\n",
    "\n",
    "# Results\n",
    "pprint(ldamodel.print_topics(num_topics=3, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8867988), (1, 0.06450699), (2, 0.048694223)]\n"
     ]
    }
   ],
   "source": [
    "doc = clean(\"My father driving my sister around to dance practice.\")\n",
    "docrep = dictionary.doc2bow(doc.split())\n",
    "pprint(ldamodel.get_document_topics(docrep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.88676494), (1, 0.06454086), (2, 0.04869421)]\n"
     ]
    }
   ],
   "source": [
    "pprint(ldamodel[docrep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.9059407), (1, 0.05158356), (2, 0.04247576)]\n"
     ]
    }
   ],
   "source": [
    "pprint(ldamodel[doc_term_matrix[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.11167304), (1, 0.76958686), (2, 0.118740074)]\n"
     ]
    }
   ],
   "source": [
    "doc = clean(\"I like sugar very much\")\n",
    "docrep = dictionary.doc2bow(doc.split())\n",
    "pprint(ldamodel[docrep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0011677861), (1, 0.051947992), (2, 0.03859802)]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_term_topics(dictionary.token2id[\"sugar\"], minimum_probability=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.453*\"driving\" + 0.383*\"sister\" + 0.331*\"school\" + 0.320*\"love\"'),\n",
      " (1, '0.460*\"school\" + 0.308*\"study\" + -0.283*\"sugar\" + -0.244*\"driving\"'),\n",
      " (2, '0.324*\"sugar\" + 0.315*\"good\" + 0.315*\"lifestyle\" + 0.305*\"health\"')]\n",
      "array([ 0.25774173, -0.20579016, -0.11838895])\n"
     ]
    }
   ],
   "source": [
    "lsa = gensim.models.lsimodel.LsiModel\n",
    "\n",
    "lsamodel = lsa(doc_term_matrix, num_topics=3, id2word = dictionary)\n",
    "\n",
    "pprint(lsamodel.print_topics(num_topics=3, num_words=4))\n",
    "pprint(lsamodel.projection.u[dictionary.token2id[\"father\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 3)\n",
      "3.2561658219633687\n"
     ]
    }
   ],
   "source": [
    "print(lsamodel.projection.u.shape)\n",
    "pprint(lsamodel.projection.s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('driving', 0.45283341984443065),\n",
       " ('sister', 0.3831712509341249),\n",
       " ('school', 0.3306286089841602),\n",
       " ('love', 0.3198014365810729),\n",
       " ('car', 0.3109918030030597)]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsamodel.show_topic(0, topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.16373040239929543), (1, -0.34217454091796345), (2, 0.343542665277609)]\n"
     ]
    }
   ],
   "source": [
    "#print(lsamodel[doc_term_matrix[1]])\n",
    "doc = clean(\"I like sugar very much\")\n",
    "docrep = dictionary.doc2bow(doc.split())\n",
    "print(lsamodel[docrep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Improvements\n",
    "* Filtering\n",
    "* Part of Speech Tag Filter\n",
    "* Chunks (Parsing)\n",
    "* NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(46 unique tokens: ['bad', 'health', 'sugar', 'around', 'car']...)\n",
      "{0: 1, 1: 3, 2: 4, 3: 1, 4: 3, 5: 1, 6: 5, 7: 3, 8: 2, 9: 4, 10: 1, 11: 4, 12: 5, 13: 3, 14: 1, 15: 1, 16: 1, 17: 1, 18: 2, 19: 1, 20: 2, 21: 2, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1, 28: 2, 29: 1, 30: 1, 31: 1, 32: 2, 33: 1, 34: 1, 35: 1, 36: 1, 37: 1, 38: 1, 39: 1, 40: 1, 41: 1, 42: 1, 43: 1, 44: 1, 45: 1}\n"
     ]
    }
   ],
   "source": [
    "tfidf = gensim.models.tfidfmodel.TfidfModel\n",
    "tfidfmodel = tfidf(doc_term_matrix, id2word = dictionary)\n",
    "print(tfidfmodel.id2word)\n",
    "print(tfidfmodel.dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 {'health': 2.415037499278844, 'sugar': 2.0, 'car': 2.415037499278844, 'driving': 1.6780719051126378, 'father': 2.415037499278844, 'practice': 3.0, 'sister': 2.0, 'love': 2.0, 'school': 1.6780719051126378, 'study': 2.415037499278844, 'well': 3.0, 'good': 3.0, 'lifestyle': 3.0, 'important': 3.0, 'life': 3.0}\n"
     ]
    }
   ],
   "source": [
    "voc = {}\n",
    "for i in range(len(tfidfmodel.id2word)):\n",
    "    if tfidfmodel.dfs[i] > 1:\n",
    "        voc[tfidfmodel.id2word[i]] = tfidfmodel.idfs[i]\n",
    "print(len(voc), voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['driving', 'school', 'sugar', 'sister', 'love', 'health', 'car', 'father', 'study', 'practice', 'well', 'good', 'lifestyle', 'important', 'life']\n"
     ]
    }
   ],
   "source": [
    "sel_features=sorted(voc, key=voc.__getitem__, reverse=False)[:15]\n",
    "print(sel_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc_clean = [[w for w in doc if w in sel_features] for doc in doc_clean]\n",
    "dictionary = corpora.Dictionary(new_doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in new_doc_clean]\n",
    "\n",
    "# Now you can create new topic models using the selected vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sugar', 'health'], ['father', 'driving', 'sister', 'practice', 'car'], ['school', 'love', 'study'], ['well', 'school'], ['health', 'sugar', 'good', 'lifestyle'], ['well', 'school', 'sister', 'study'], ['father', 'driving'], ['health', 'important', 'sugar'], ['good', 'lifestyle', 'life'], ['life', 'important', 'practice'], ['love', 'driving', 'car'], ['driving', 'love', 'car'], ['driving', 'sister'], ['study', 'love', 'school'], ['school'], ['sister', 'sugar', 'father']]\n"
     ]
    }
   ],
   "source": [
    "print(new_doc_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing LDA and LSA again\n",
    "#### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.205*\"sugar\" + 0.159*\"health\" + 0.111*\"good\" + 0.111*\"lifestyle\"'),\n",
      " (1, '0.296*\"school\" + 0.185*\"study\" + 0.131*\"love\" + 0.130*\"well\"'),\n",
      " (2, '0.242*\"driving\" + 0.152*\"sister\" + 0.151*\"car\" + 0.151*\"father\"')]\n"
     ]
    }
   ],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=100)\n",
    "pprint(ldamodel.print_topics(num_topics=3, num_words=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '-0.502*\"driving\" + -0.385*\"sister\" + -0.382*\"love\" + -0.359*\"car\"'),\n",
      " (1, '-0.529*\"school\" + -0.382*\"study\" + 0.370*\"sugar\" + 0.283*\"health\"'),\n",
      " (2, '-0.501*\"sugar\" + -0.449*\"health\" + 0.336*\"driving\" + -0.325*\"school\"')]\n"
     ]
    }
   ],
   "source": [
    "lsa = gensim.models.lsimodel.LsiModel\n",
    "lsamodel = lsa(doc_term_matrix, num_topics=3, id2word = dictionary)\n",
    "\n",
    "pprint(lsamodel.print_topics(num_topics=3, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 3)\n"
     ]
    }
   ],
   "source": [
    "print(lsamodel.projection.u.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.chunk\n",
    "def conll_tag_chunks(chunk_sents):\n",
    "    tag_sents = [nltk.chunk.tree2conlltags(tree) for tree in chunk_sents]\n",
    "    return [[(t, c) for (w, t, c) in chunk_tags] for chunk_tags in tag_sents]\n",
    "\n",
    "conll_train = nltk.corpus.conll2000.chunked_sents('train.txt')\n",
    "conll_test = nltk.corpus.conll2000.chunked_sents('test.txt')\n",
    "train_chunks = conll_tag_chunks(conll_train)\n",
    "ubt_chunker = nltk.tag.TrigramTagger(train_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NNP'), ('ate', 'VBP'), ('an', 'DT'), ('apple', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'John ate an apple'\n",
    "pos_tags = nltk.pos_tag(sentence.split())\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NNP', 'B-NP'), ('VBP', 'B-VP'), ('DT', 'B-NP'), ('NN', 'I-NP')]\n"
     ]
    }
   ],
   "source": [
    "chunks = ubt_chunker.tag([t for w,t in pos_tags])\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['John', 'ate', 'an_apple'], ['the_computer', 'is', 'in', 'the_machine_room']]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_chunks(docs):\n",
    "    newdocs=[]\n",
    "    for doc in docs:\n",
    "        tags = nltk.pos_tag(nltk.word_tokenize(doc))\n",
    "        chunks = ubt_chunker.tag([t for w,t in tags])\n",
    "        phrase=[]\n",
    "        for i in range(len(tags)):\n",
    "            #print(tags[i], chunks[i])\n",
    "            if chunks[i][1] == None or not chunks[i][1].startswith(\"I\"):\n",
    "                phrase.append(\" \"+tags[i][0])\n",
    "            else:\n",
    "                phrase.append(\"_\"+tags[i][0])\n",
    "        newdocs.append(\"\".join(phrase).split())\n",
    "    return newdocs\n",
    "\n",
    "get_chunks(['John ate an apple', 'the computer is in the machine room'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['john', 'ate', 'an_apple'], ['the_computer', 'the_machine_room']]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "def clean2(docs):\n",
    "    res = []\n",
    "    for doc in docs:\n",
    "        punc_free = ' '.join(ch for ch in doc if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        res.append([w for w in normalized.lower().split() if w not in stop])\n",
    "    return res\n",
    "\n",
    "chunks = get_chunks(['John ate an apple', 'the computer is in the machine room'])\n",
    "clean2(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sugar', 'bad', 'health'], ['my_father', 'is_driving', 'my_sister', 'around', 'dance_practice', 'car'], ['my_school', 'greate', 'love_to_study'], ['sometimes', 'feel', 'happy', 'to_perform', 'well', 'school'], ['health_experts', 'say', 'sugar', 'is_not', 'good', 'your_lifestyle'], ['am_doing', 'well', 'school', 'my_sister', 'could_study', 'little'], ['my_father', 'does_not_need_driving'], ['my_health', 'important', \"do_n't_use\", 'sugar'], ['a_good_lifestyle', 'mean', 'le', 'blood', 'pressure', 'a_long_life'], ['my_life', 'important', 'i_practice_sports'], ['love_driving', 'my_car'], ['am_driving_to_relax', 'really', 'love', 'my_car'], ['driving', 'my_sister_home', 'realy', 'nice'], ['study', 'everyday', 'love', 'the_school'], ['my_school', 'the_best'], ['my_sister_likes', 'to_have', 'sugar', 'my_father']]\n"
     ]
    }
   ],
   "source": [
    "doc_clean = clean2(get_chunks(doc_complete))\n",
    "print(doc_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing LDA and LSA again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)], [(3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)], [(9, 1), (10, 1), (11, 1)], [(12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)], [(2, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)], [(8, 1), (14, 1), (17, 1), (23, 1), (24, 1), (25, 1)], [(7, 1), (26, 1)], [(2, 1), (27, 1), (28, 1), (29, 1)], [(30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1)], [(28, 1), (36, 1), (37, 1)], [(38, 1), (39, 1)], [(39, 1), (40, 1), (41, 1), (42, 1)], [(43, 1), (44, 1), (45, 1), (46, 1)], [(41, 1), (47, 1), (48, 1), (49, 1)], [(11, 1), (50, 1)], [(2, 1), (7, 1), (51, 1), (52, 1)]]\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.078*\"my_car\" + 0.046*\"my_father\" + 0.045*\"love\" + 0.045*\"my_sister\"'),\n",
      " (1, '0.092*\"sugar\" + 0.050*\"my_father\" + 0.029*\"important\" + 0.029*\"good\"'),\n",
      " (2, '0.056*\"well\" + 0.056*\"school\" + 0.033*\"my_school\" + 0.032*\"sometimes\"')]\n"
     ]
    }
   ],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "pprint(ldamodel.print_topics(num_topics=3, num_words=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '-0.418*\"well\" + -0.418*\"school\" + -0.366*\"my_sister\" + -0.245*\"my_father\"'),\n",
      " (1, '0.621*\"sugar\" + 0.236*\"my_father\" + 0.226*\"is_not\" + 0.226*\"say\"'),\n",
      " (2,\n",
      "  '-0.446*\"my_father\" + -0.306*\"is_driving\" + -0.306*\"dance_practice\" + '\n",
      "  '-0.306*\"car\"')]\n"
     ]
    }
   ],
   "source": [
    "lsa = gensim.models.lsimodel.LsiModel\n",
    "lsamodel = lsa(doc_term_matrix, num_topics=3, id2word = dictionary)\n",
    "pprint(lsamodel.print_topics(num_topics=3, num_words=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrinsic Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherence Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.5267428131262261\n"
     ]
    }
   ],
   "source": [
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.4974575333202984\n"
     ]
    }
   ],
   "source": [
    "coherence_model_lsa = CoherenceModel(model=lsamodel, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lsa = coherence_model_lsa.get_coherence()\n",
    "print('Coherence Score: ', coherence_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
