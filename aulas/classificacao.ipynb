{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn: simple classification example\n",
    "More details in http://scikit-learn.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "This data sets consists of 3 different types of irisesâ€™ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray.\n",
    "\n",
    "The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gnb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((y!=y_pred).sum(), X.shape[0]))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(y, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(y, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(y, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter=200)\n",
    "model=lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((y!=y_pred).sum(), X.shape[0]))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(y, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(y, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(y, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "model = clf.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((y!=y_pred).sum(), X.shape[0]))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(y, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(y, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(y, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=('ham', 'spam')\n",
    "textos = []\n",
    "y = []\n",
    "\n",
    "with open(\"../data/en/SMSSpamCollection.txt\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=\"\\t\")\n",
    "    for i, row in enumerate(reader):\n",
    "        if row[\"class\"] == classes[1]:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "        textos.append(row[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "docs = []\n",
    "\n",
    "for t in textos:\n",
    "    doc = collections.Counter()\n",
    "    for w in tokenizer.tokenize(t):\n",
    "        doc[w] += 1\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_length = 3000\n",
    "\n",
    "tf = collections.Counter()\n",
    "df = collections.Counter()\n",
    "\n",
    "for d in docs:\n",
    "    for w in d:\n",
    "        tf[w] += d[w]\n",
    "        df[w] += 1\n",
    "\n",
    "idfs = {}\n",
    "for w in tf:\n",
    "    if tf[w] > 2:\n",
    "        idfs[w] = np.log(len(docs)/df[w])\n",
    "\n",
    "voc = sorted(idfs, key=idfs.get, reverse=True)[:voc_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice = {}\n",
    "for i,w in enumerate(sorted(voc)):\n",
    "    indice[w] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docrep = []\n",
    "for d in docs:\n",
    "    valores = np.zeros([len(voc)])\n",
    "    for w in d:\n",
    "        if w in indice:\n",
    "            valores[ indice[w] ] = d[w]\n",
    "    docrep.append ( valores )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docrep[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "nb = MultinomialNB()\n",
    "model = nb.fit(docrep, y)\n",
    "y_pred = model.predict(docrep)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((y!=y_pred).sum(), len(docrep)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(y, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(y, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(y, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "lr = LogisticRegression()\n",
    "model=lr.fit(docrep, y)\n",
    "y_pred = model.predict(docrep)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((y!=y_pred).sum(), len(docrep)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(y, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(y, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(y, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svmc = LinearSVC(max_iter=500)\n",
    "model = svmc.fit(docrep, y)\n",
    "y_pred = model.predict(docrep)\n",
    "print(\"Mislabeled points: %d out of %d\"% ((y!=y_pred).sum(), len(docrep)))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y, y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(y, y_pred, average=\"macro\"))\n",
    "print(\"Recall: \", metrics.recall_score(y, y_pred, average=\"macro\"))\n",
    "print(\"F1-measure: \", metrics.f1_score(y, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NLTK\n",
    "\n",
    "## A very simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.classify import maxent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\n",
    " ({'a': 1, 'b': 1, 'c': 1}, 'y'),\n",
    " ({'a': 5, 'b': 5, 'c': 5}, 'x'),\n",
    " ({'a': 0.9, 'b': 0.9, 'c': 0.9}, 'y'),\n",
    " ({'a': 5.5, 'b': 5.4, 'c': 5.3}, 'x'),\n",
    " ({'a': 0.8, 'b': 1.2, 'c': 1}, 'y'),\n",
    " ({'a': 5.1, 'b': 4.9, 'c': 5.2}, 'x')\n",
    "]\n",
    "test = [\n",
    "  {'a': 1, 'b': 0.8, 'c': 1.2},\n",
    "  {'a': 5.2, 'b': 5.1, 'c': 5}\n",
    "]\n",
    "classifier = maxent.MaxentClassifier.train(train, bernoulli=False, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.classify.accuracy(classifier,train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.classify_many(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK for text classification\n",
    "\n",
    "Assuming that the document is already processed (see above Document Representation steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.classify import naivebayes\n",
    "from nltk.classify import maxent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdocrep = []\n",
    "for d,c in zip(docs, y):\n",
    "    docwords={}\n",
    "    for w in d:\n",
    "        if w in indice:\n",
    "            docwords[w] = d[w]\n",
    "    newdocrep.append ( (docwords, classes[c] ) )\n",
    "newdocrep[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc = naivebayes.NaiveBayesClassifier.train(newdocrep)\n",
    "nltk.classify.accuracy(nbc,newdocrep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Entropy (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mec = maxent.MaxentClassifier.train(newdocrep, bernoulli=False,max_iter=10, trace=3)\n",
    "nltk.classify.accuracy(mec,newdocrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
