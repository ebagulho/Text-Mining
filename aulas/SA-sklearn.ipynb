{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4520835-0e3c-430d-a665-fe6e60c44f81",
   "metadata": {},
   "source": [
    "# Training your own Sentiment Classifier from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41947640-c329-40bc-a7be-1dfb4338b926",
   "metadata": {
    "tags": []
   },
   "source": [
    "Lets use [Sentiment Polarity Dataset 2.0](https://www.cs.cornell.edu/people/pabo/movie-review-data/), included in the `NLTK` library. It consists of 1000 positive and 1000 negative processed reviews. Introduced in Pang/Lee ACL 2004. Released June 2004.\n",
    "\n",
    "We will be using scikit-learn , a very efficient library for text processing and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73c523-fef6-441b-804f-59fb1fea77a9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4e4229",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe89e0-7f4a-4722-8f1e-9e8e154ebd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download(['movie_reviews','punkt','stopwords'])\n",
    "\n",
    "# Let's use some useful scikit-learn functions \n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71815e37-102c-4f5a-b248-0c9402bcc3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "print(\"The data contains %d reviews\"% len(mr.fileids()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41155b81-6bb7-466b-8b2e-528b9a59136c",
   "metadata": {},
   "source": [
    "### Shuffling the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93103b73-1df5-4f61-a9ce-0ac720fdac17",
   "metadata": {},
   "source": [
    "We start by shuffling the documents, otherwise they will remain sorted [\"neg\", \"neg\" ... \"pos\"]. Then we will proceed using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180224ac-8664-4d7d-b519-345bbeb0ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "docnames=mr.fileids()\n",
    "random.shuffle(docnames)\n",
    "\n",
    "# create two separate lists: documents and tags\n",
    "documents=[]\n",
    "tags = []\n",
    "for doc in docnames:\n",
    "    documents.append(mr.raw(doc))\n",
    "    tags.append( doc.split('/')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492a452a-9218-433e-9d39-33df0c6cd48a",
   "metadata": {},
   "source": [
    "Let's check the first few documents ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb83ae-15d9-46ef-b9f4-10ee0e7e9a4c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"DOC:\", documents[i][:400])\n",
    "    print(\"TAG:\", tags[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86403678-6aff-4476-ae6e-e3ac494d4673",
   "metadata": {},
   "source": [
    "The first 80% of the documents will be used for training, and the final 20% will be used for testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b67ffd-07df-4711-8f7f-2a5f0931507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numtrain = int(len(documents) * 80 / 100)  # number of training documents\n",
    "train_documents, test_documents = documents[:numtrain], documents[numtrain:]\n",
    "train_tags, test_tags = tags[:numtrain], tags[numtrain:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df12f4-595a-4566-9f7d-cf072717330f",
   "metadata": {},
   "source": [
    "Now that we have separated training and testing sets, we will convert the texts into their vectorial representation. Scikit-learn provides two interesting methods for this: `CountVectorizer` and `TfidfVectorizer`. Please check the documentation if you want to check different parameters.\n",
    "- [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "- [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa5d7cc-fc44-4110-821a-42af80a84026",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_X = vectorizer.fit_transform(train_documents)\n",
    "test_X = vectorizer.transform(test_documents)\n",
    "print(\"TRAIN SIZE:\", train_X.shape)\n",
    "print(\"TEST SIZE:\", test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c6062-2e6d-47f9-b0bc-2d1c421093e3",
   "metadata": {},
   "source": [
    "We can see that the features are actually the words from the texts, where some strange \"words\" can also be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ca76f-4802-4104-8b66-8eb7bfe24a3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()[600:700]\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88006179-ab4a-47cf-81e1-560fc9149e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c14cc-67c4-484e-9a33-da08f974ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(train_X, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be3c92-c2f7-4469-b6c7-bd1c434c3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923a7bc-294c-4feb-9975-bd911d748907",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = sklearn.metrics.accuracy_score(test_tags, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7214ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_tags, pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea54cf7-89d3-4c8e-ad07-b9d52b79baa2",
   "metadata": {},
   "source": [
    "### Using the classifier for processing new texts\n",
    "\n",
    "1. Please note that you have to perform the exact same processing steps to the new sentences, previously used during training.\n",
    "2. Then, you have only to apply the classifier to the new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e760d13-e44f-4dbb-85cd-3ee533678cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = [\"I love movies very much\", \n",
    "          \"I hate my stupid life\",\n",
    "          \"I am disapointed with the argument\"]\n",
    "frases_X = vectorizer.transform(frases)\n",
    "classifier.predict(frases_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
