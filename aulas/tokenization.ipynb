{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization \n",
    "\n",
    "This notebook focuses on word-based tokenization.\n",
    "\n",
    "For sub-word tokenization, Please check [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/quicktour) for an overview, and for easily training your own tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso esteja a utilizar o google colab, deve seguir as instruções disponíveis no moodle\n",
    "e correr a célula seguinte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Espirais do Polo Norte de Marte mostradas em detalhe\n",
    "A Agência Espacial Europeia pegou em 32 \"tiras\" de órbitas registadas pela sonda Mars Express entre 2004 e 2010 e criou um \"mosaico\" que cobre uma área de cerca de um milhão de quilómetros quadrados.\n",
    "\"Esta informação é muito importante para perceber como evoluiu o clima no planeta à medida que a sua inclinação e órbita variavam ao longo de centenas ou mesmo milhares de anos\", refere a ESA.\n",
    "A Estação Espacial Europeia acredita que são os ventos fortes da região os responsáveis por moldarem o gelo, já que sopram desde a parte central mais alta, até às margens inferiores e fazem redemoinhos empurrados pela mesma força que faz os furacões girarem na Terra.\n",
    "Ainda que a calote polar seja um \"elemento permanente\", durante o inverno as temperaturas são tão baixas que 30% do dióxido de carbono da atmosfera do planeta precipita-se sobre a mesma, acrescentando-lhe uma \"capa extra\" de até um metro de espessura.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantas palavras diferentes podemos encontrar neste texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tokenização baseada em palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Formas básicas de identificar os *tokens* de um texto\n",
    "* Usar os espaços"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que há tokens que fazem pouco sentido: e.g. `\"mosaico\"`, `quadrados.`, ...\n",
    "\n",
    "Uma forma de resolver o problema poderá envolver processamento do texto, por exemplo separando os carateres `\"` e `.` das palavras à sua volta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = re.sub(r'([.\\\"])', ' \\\\1 ', text)\n",
    "text2 = re.sub(r' +', ' ', text2)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora sim, já será possivel obter uma segmentação mais interessante..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text2.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Façamos agora algum pré-processamento básico inicial para juntar as várias frases numa única linha e transformar tudo em minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = re.sub(r'[\\n\\r]', \"\", text2).lower()\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois do pré-processamento inicial, calcular a frequência de cada token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "freq = collections.Counter(text2.split())\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq[\"que\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (opcional) Uso de expressões regulares\n",
    "\n",
    "As expresssões regulares têm inúmeras aplicações: filtragem, obtenção de padões, etc.\n",
    "\n",
    "Options:\n",
    "- `re.I`: perform case-insensitive matching\n",
    "- `re.MULTILINE`: When specified, the pattern character '^' matches at the beginning of the string and at the beginning of each line (immediately following each newline)\n",
    "- `re.DOTALL`: Make the '.' special character match any character at all, including a newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = re.findall(r\"^[A-Z][a-z]+\", text, re.MULTILINE)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = re.findall(r\"^\\w+\", text, re.MULTILINE)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outros testes: por exemplo, identificar todas as sequências de letras ou números\n",
    "print(text)\n",
    "palavras = re.sub(r\"(\\w+)\", '<\\\\1>', text)\n",
    "print(palavras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização usando NLTK\n",
    "Observe que para além dos sinais de pontuação, tais como \".\" e \",\", as aspas também podem representar um problema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamento de casos particulares, tais como abreviaturas, siglas, acrónimos e quantidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"Dr. Peter mentioned that the U.S.A. poster-print costs $12.40...\"\n",
    "tokens=nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização de Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mesma solução não produz um bom resultado quando aplicada, por exemplo, a tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "\n",
    "print(nltk.word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "mytkzr = TweetTokenizer()\n",
    "print(mytkzr.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro exemplo ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"Olá @manuel_silva, #text-mining rulezzzz. :)\"\n",
    "print(mytkzr.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplos adicionais com ficheiros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de continuar, certifique-se que fez o download dos dados previamente conforme instruções fornecidas em [download dos dados utilizados nos exemplos](./using-tm-data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "with gzip.open(\"../data/pt/tsf.selecionado/not_4180883.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    text  = \"\".join(lines)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "words = nltk.word_tokenize(text)\n",
    "print(\" | \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
