{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento básico de texto\n",
    "## Como identificar os tokens de um texto ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texto=\"\"\"Espirais do Polo Norte de Marte mostradas em detalhe\n",
    "A Agência Espacial Europeia pegou em 32 \"tiras\" de órbitas registadas pela sonda Mars Express entre 2004 e 2010 e criou um \"mosaico\" que cobre uma área de cerca de um milhão de quilómetros quadrados.\n",
    "\"Esta informação é muito importante para perceber como evoluiu o clima no planeta à medida que a sua inclinação e órbita variavam ao longo de centenas ou mesmo milhares de anos\", refere a ESA.\n",
    "A Estação Espacial Europeia acredita que são os ventos fortes da região os responsáveis por moldarem o gelo, já que sopram desde a parte central mais alta, até às margens inferiores e fazem redemoinhos empurrados pela mesma força que faz os furacões girarem na Terra.\n",
    "Ainda que a calote polar seja um \"elemento permanente\", durante o inverno as temperaturas são tão baixas que 30% do dióxido de carbono da atmosfera do planeta precipita-se sobre a mesma, acrescentando-lhe uma \"capa extra\" de até um metro de espessura.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantas palavras diferentes podemos encontrar neste texto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claro que podemos eliminar as palavras repetidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(texto.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texto2 = re.sub(r\"([\\,\\.\\\"])\", r\" \\1 \", texto).lower()\n",
    "texto2=re.sub(r\"[\\n\\r]\", \"\", texto2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(texto2.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "freq=collections.Counter(texto2.split())\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq[\"que\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agora usando o NLTK\n",
    "Pode-se ver que alguns sinais de pontuação, tais como \".\" e \",\" são corretamente separados. No entanto, as aspas representam um problema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "palavras=nltk.word_tokenize(texto.lower())\n",
    "set(palavras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização de Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "mytkzr = TweetTokenizer()\n",
    "\n",
    "tweet = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "mytkzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro exemplo ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet = \"Olá @iscte, a cadeira de #text-mining rulezzzz. :)\"\n",
    "mytkzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de processamento de um ficheiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "f=gzip.open(\"../data/pt/tsf.selecionado/not_4180883.gz\", \"rt\", encoding=\"utf-8\")\n",
    "texto=f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t=\"\\n\".join(texto[2:])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "palavras=nltk.word_tokenize(t)\n",
    "print(\" | \".join(palavras))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem as stem\n",
    "sentence = \"the flies died and denied their dead stating sensational lies\"\n",
    "\n",
    "stemmer = stem.porter.PorterStemmer()\n",
    "res = [ stemmer.stem(w) for w in sentence.split()]\n",
    "print(\" \".join(res) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos também `stemmers` mais sofisticados. Por exemplo, o SnowballStemmer também funciona para Português (apesar de ser mauzito) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Languages:\", stem.snowball.SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frase = \"lindos são os prados verdes e com muita luz deste Portugal\"\n",
    "stemmer = stem.snowball.SnowballStemmer(\"portuguese\")\n",
    "res = [stemmer.stem(w) for w in frase.split()]\n",
    "print(\" \".join(res) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob for sentence segmentation\n",
    "TextBlob is a new python natural language processing toolkit, which stands on the shoulders of giants like NLTK and Pattern, provides text mining, text analysis and text processing modules for python developers.<BR>\n",
    "Material baseado em: http://textminingonline.com/getting-started-with-textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from pprint import pprint\n",
    "\n",
    "text = \"\"\"Natural language processing (NLP) deals with the application of computational models to text or speech data.\n",
    "Application areas within NLP include automatic (machine) translation between languages; dialogue systems, which allow a human to interact with a machine using natural language; and information extraction, where the goal is to transform unstructured text into structured (database) representations that can be searched and browsed in flexible ways.\"\"\"\n",
    "\n",
    "blob = TextBlob(text)\n",
    "pprint(blob.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for frase in blob.sentences:\n",
    "    print(frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
